{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3cd0e1b",
   "metadata": {},
   "source": [
    "# Week 8 - K-Nearest Neighbors and distance metrics\n",
    "\n",
    "Euclidean, Manhattan, Minkowski, Cosine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3604ba66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting statsmodels\n",
      "  Downloading statsmodels-0.14.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.22.3 in /home/codespace/.local/lib/python3.12/site-packages (from statsmodels) (2.3.1)\n",
      "Requirement already satisfied: scipy!=1.9.2,>=1.8 in /home/codespace/.local/lib/python3.12/site-packages (from statsmodels) (1.16.0)\n",
      "Requirement already satisfied: pandas!=2.1.0,>=1.4 in /home/codespace/.local/lib/python3.12/site-packages (from statsmodels) (2.3.1)\n",
      "Collecting patsy>=0.5.6 (from statsmodels)\n",
      "  Downloading patsy-1.0.2-py2.py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: packaging>=21.3 in /home/codespace/.local/lib/python3.12/site-packages (from statsmodels) (25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.12/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.12/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels) (1.17.0)\n",
      "Downloading statsmodels-0.14.5-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading patsy-1.0.2-py2.py3-none-any.whl (233 kB)\n",
      "Installing collected packages: patsy, statsmodels\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2/2\u001b[0m [statsmodels]\u001b[0m [statsmodels]\n",
      "\u001b[1A\u001b[2KSuccessfully installed patsy-1.0.2 statsmodels-0.14.5\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81e466f2-afc8-4d96-b87c-bf77d255efb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import statsmodels.api as sm\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7ddb5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8116f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 1\n",
    "df = pd.read_csv(\"diabetes_012_health_indicators_BRFSS2015.csv\")\n",
    "\n",
    "# Dataset 2\n",
    "df_pima = pd.read_csv(\"pima_indian_diabetes_dataset.csv\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d63dbc-c416-4795-b467-2aac90e94936",
   "metadata": {},
   "source": [
    "# Dataset 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddb2ef8-8dd7-4a31-b4f7-2bda4f8a2dc6",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors with the following distance metrics: Euclidean, Manhattan, Minkowski, Cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84452e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Distance Metric: EUCLIDEAN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1953/1625790812.py:14: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.sample(n=5000, random_state=42, replace=True))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Accuracy: 0.501 Â± 0.013\n",
      "Test Accuracy: 0.509\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.52      0.59      0.55      1000\n",
      "         1.0       0.49      0.52      0.50      1000\n",
      "         2.0       0.53      0.41      0.46      1000\n",
      "\n",
      "    accuracy                           0.51      3000\n",
      "   macro avg       0.51      0.51      0.51      3000\n",
      "weighted avg       0.51      0.51      0.51      3000\n",
      "\n",
      "\n",
      "ðŸ”¹ Distance Metric: MANHATTAN\n",
      "Cross-Validation Accuracy: 0.508 Â± 0.013\n",
      "Test Accuracy: 0.524\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.53      0.61      0.57      1000\n",
      "         1.0       0.49      0.53      0.51      1000\n",
      "         2.0       0.56      0.43      0.49      1000\n",
      "\n",
      "    accuracy                           0.52      3000\n",
      "   macro avg       0.53      0.52      0.52      3000\n",
      "weighted avg       0.53      0.52      0.52      3000\n",
      "\n",
      "\n",
      "ðŸ”¹ Distance Metric: MINKOWSKI\n",
      "Cross-Validation Accuracy: 0.501 Â± 0.013\n",
      "Test Accuracy: 0.509\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.52      0.59      0.55      1000\n",
      "         1.0       0.49      0.52      0.50      1000\n",
      "         2.0       0.53      0.41      0.46      1000\n",
      "\n",
      "    accuracy                           0.51      3000\n",
      "   macro avg       0.51      0.51      0.51      3000\n",
      "weighted avg       0.51      0.51      0.51      3000\n",
      "\n",
      "\n",
      "ðŸ”¹ Distance Metric: COSINE\n",
      "Cross-Validation Accuracy: 0.501 Â± 0.015\n",
      "Test Accuracy: 0.507\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.53      0.59      0.56      1000\n",
      "         1.0       0.47      0.52      0.49      1000\n",
      "         2.0       0.53      0.41      0.46      1000\n",
      "\n",
      "    accuracy                           0.51      3000\n",
      "   macro avg       0.51      0.51      0.50      3000\n",
      "weighted avg       0.51      0.51      0.50      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- KNN Classification on Balanced 10,000-Sample (Dataset 1) ---\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Create a balanced 10,000-row sample ---\n",
    "df_small = (\n",
    "    df.groupby('Diabetes_012', group_keys=False)\n",
    "      .apply(lambda x: x.sample(n=5000, random_state=42, replace=True))\n",
    ")\n",
    "\n",
    "\n",
    "X = df_small.drop(columns=['Diabetes_012'])\n",
    "y = df_small['Diabetes_012']\n",
    "\n",
    "# --- 2. Scale features ---\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# --- 3. Split data ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# --- 4. Evaluate multiple distance metrics ---\n",
    "distance_metrics = ['euclidean', 'manhattan', 'minkowski', 'cosine']\n",
    "\n",
    "for metric in distance_metrics:\n",
    "    print(f\"\\nðŸ”¹ Distance Metric: {metric.upper()}\")\n",
    "    knn = KNeighborsClassifier(n_neighbors=5, metric=metric)\n",
    "    \n",
    "    # 5-fold cross-validation\n",
    "    cv_scores = cross_val_score(knn, X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    print(f\"Cross-Validation Accuracy: {cv_scores.mean():.3f} Â± {cv_scores.std():.3f}\")\n",
    "    \n",
    "    # Fit and test\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    print(f\"Test Accuracy: {accuracy_score(y_test, y_pred):.3f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8e908b",
   "metadata": {},
   "source": [
    "I used 5,000 samples per group becuase thw full 200,000 dataset took too long. If I ran with 33,333 per group it had an error with memory. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e462eba",
   "metadata": {},
   "source": [
    "# Dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4de299a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”¹ Distance Metric: EUCLIDEAN\n",
      "Cross-Validation Accuracy: 0.733 Â± 0.014\n",
      "Test Accuracy: 0.708\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.80      0.78       100\n",
      "           1       0.59      0.54      0.56        54\n",
      "\n",
      "    accuracy                           0.71       154\n",
      "   macro avg       0.68      0.67      0.67       154\n",
      "weighted avg       0.70      0.71      0.70       154\n",
      "\n",
      "\n",
      "ðŸ”¹ Distance Metric: MANHATTAN\n",
      "Cross-Validation Accuracy: 0.730 Â± 0.026\n",
      "Test Accuracy: 0.734\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.83      0.80       100\n",
      "           1       0.64      0.56      0.59        54\n",
      "\n",
      "    accuracy                           0.73       154\n",
      "   macro avg       0.71      0.69      0.70       154\n",
      "weighted avg       0.73      0.73      0.73       154\n",
      "\n",
      "\n",
      "ðŸ”¹ Distance Metric: MINKOWSKI\n",
      "Cross-Validation Accuracy: 0.733 Â± 0.014\n",
      "Test Accuracy: 0.708\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.80      0.78       100\n",
      "           1       0.59      0.54      0.56        54\n",
      "\n",
      "    accuracy                           0.71       154\n",
      "   macro avg       0.68      0.67      0.67       154\n",
      "weighted avg       0.70      0.71      0.70       154\n",
      "\n",
      "\n",
      "ðŸ”¹ Distance Metric: COSINE\n",
      "Cross-Validation Accuracy: 0.761 Â± 0.038\n",
      "Test Accuracy: 0.714\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.77      0.78       100\n",
      "           1       0.59      0.61      0.60        54\n",
      "\n",
      "    accuracy                           0.71       154\n",
      "   macro avg       0.69      0.69      0.69       154\n",
      "weighted avg       0.72      0.71      0.72       154\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- KNN Classification with Multiple Distance Metrics (Dataset 2) ---\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Prepare data ---\n",
    "X = df_pima.drop(columns=['Outcome'])\n",
    "y = df_pima['Outcome']\n",
    "\n",
    "# --- 2. Scale features ---\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# --- 3. Train/test split ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# --- 4. Define distance metrics to test ---\n",
    "distance_metrics = ['euclidean', 'manhattan', 'minkowski', 'cosine']\n",
    "\n",
    "# --- 5. Run KNN for each distance metric ---\n",
    "for metric in distance_metrics:\n",
    "    print(f\"\\nðŸ”¹ Distance Metric: {metric.upper()}\")\n",
    "    \n",
    "    knn = KNeighborsClassifier(n_neighbors=5, metric=metric)\n",
    "    \n",
    "    # Cross-validation accuracy (5 folds)\n",
    "    cv_scores = cross_val_score(knn, X_train, y_train, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    print(f\"Cross-Validation Accuracy: {cv_scores.mean():.3f} Â± {cv_scores.std():.3f}\")\n",
    "    \n",
    "    # Fit on full training data\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    # Test set evaluation\n",
    "    y_pred = knn.predict(X_test)\n",
    "    print(f\"Test Accuracy: {accuracy_score(y_test, y_pred):.3f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5597812",
   "metadata": {},
   "source": [
    "NOTES \n",
    "\n",
    "KNN doesnâ€™t handle imbalance well â€” itâ€™s biased toward the majority class.\n",
    "- what I did: stratify=y --> Keeps class proportions consistent across train/test, it's a metric in train test split, \"Ensures your train/test sets preserve class proportions.\"\n",
    "- can do weighted KNN: KNeighborsClassifier(weights='distance') â€” easy and effective. \"Gives closer neighbors more influence than distant ones.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e5708a",
   "metadata": {},
   "source": [
    "| Dataset                      | Situation                              | Recommendation                                                                                               |\n",
    "| ---------------------------- | -------------------------------------- | ------------------------------------------------------------------------------------------------------------ |\n",
    "| **Dataset 1 (Diabetes_012)** | Heavily imbalanced (class 0 dominates) | ðŸ”¹ Use **resampling** or **weights='distance'** in KNN.<br>ðŸ”¹ Keep your 10 k balanced subset (already good). |\n",
    "| **Dataset 2 (Pima)**         | More balanced, smaller (~700 rows)     | âœ… Stratification is enough â€” no resampling needed.                                                           |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b098b98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you want to use weighted KNN to reduce imbalance bias:\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5, metric='euclidean', weights='distance')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe70e075",
   "metadata": {},
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5, metric='euclidean', weights='distance')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdda761",
   "metadata": {},
   "source": [
    "increasing k = The model becomes smoother (less sensitive to noise). Accuracy may improve slightly, but it can underfit if k is too large (loses local detail).\n",
    "\n",
    "decreasing k = The model becomes more flexible and fits local variations better, but can overfit and become noisy. k = 1 memorizes the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c63433f",
   "metadata": {},
   "source": [
    "Typical sweet spot: k â‰ˆ 3â€“10.\n",
    "You can tune it with GridSearchCV(param_grid={'n_neighbors': range(3, 21)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecb921a",
   "metadata": {},
   "source": [
    "TO DO \n",
    "\n",
    "- analyze the output results and compare to models from previous week \n",
    "- test different k numbers \n",
    "- tune k "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
