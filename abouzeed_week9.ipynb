{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "bd3e4179-0a4e-47c4-a778-a2afc8d2f0fe",
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\nfrom sklearn.inspection import permutation_importance\n\nUSE_SYNTHETIC = True         \nCSV_PATH = \"path/to/your.csv\"\nTARGET_COL = \"target\"\n\nRANDOM_SEED = 42\nTEST_SIZE   = 0.25\n\n# 1) Data\ndef make_synthetic_real_estate(n=1500, seed=42):\n    rng = np.random.default_rng(seed)\n    sqft = rng.normal(1800, 450, size=n).clip(400, 5000)\n    bed  = rng.integers(1, 6, size=n)\n    loc  = rng.choice([\"A\",\"B\",\"C\"], size=n, p=[0.45, 0.4, 0.15])\n\n    # true signal (hundreds of thousands)\n    loc_eff = {\"A\":0.0, \"B\":0.25, \"C\":0.6}\n    noise = rng.normal(0, 0.18, size=n)\n    y = 0.0012*sqft + 0.17*bed + np.vectorize(loc_eff.get)(loc) + noise\n\n    X = pd.DataFrame({\"sqft\": sqft, \"bed\": bed, \"loc\": loc})\n    y = pd.Series(y, name=\"price\")\n    return X, y\n\nif USE_SYNTHETIC:\n    X, y = make_synthetic_real_estate()\n    print(\"Using synthetic data:\", X.shape, \"Target = price\")\nelse:\n    df = pd.read_csv(CSV_PATH)\n    if TARGET_COL not in df.columns:\n        raise ValueError(f\"'{TARGET_COL}' not found in CSV columns.\")\n    y = df[TARGET_COL]\n    X = df.drop(columns=[TARGET_COL])\n    print(\"Using CAPSTONE data:\", X.shape, \"Target =\", TARGET_COL)\n\n# 2) Split & Preprocess\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=TEST_SIZE, random_state=RANDOM_SEED\n)\n\ncat_cols = [c for c in X.columns if X[c].dtype == 'object']\nnum_cols = [c for c in X.columns if X[c].dtype != 'object']\n\npreprocess = ColumnTransformer(\n    transformers=[\n        (\"num\", Pipeline([(\"imp\", SimpleImputer(strategy=\"median\"))]), num_cols),\n        (\"cat\", Pipeline([\n            (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n            (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\"))\n        ]), cat_cols),\n    ]\n)\n\nprint(\"Numeric columns:\", num_cols)\nprint(\"Categorical columns:\", cat_cols)\n\n# 3) Baselines: Mean & Random Forest\ny_pred_mean = np.full_like(np.asarray(y_test, dtype=float), fill_value=float(np.mean(y_train)))\nmae_mean  = mean_absolute_error(y_test, y_pred_mean)\nrmse_mean = mean_squared_error(y_test, y_pred_mean, squared=False)\nprint(f\"Baseline (mean)  MAE={mae_mean:.4f}  RMSE={rmse_mean:.4f}\")\n\nrf = Pipeline([\n    (\"prep\", preprocess),\n    (\"rf\", RandomForestRegressor(\n        n_estimators=400,\n        max_depth=None,\n        min_samples_leaf=1,\n        random_state=RANDOM_SEED,\n        n_jobs=-1\n    ))\n])\nrf.fit(X_train, y_train)\nrf_pred = rf.predict(X_test)\nrf_mae  = mean_absolute_error(y_test, rf_pred)\nrf_rmse = mean_squared_error(y_test, rf_pred, squared=False)\nprint(f\"RandomForest     MAE={rf_mae:.4f}  RMSE={rf_rmse:.4f}\")\n\n# 4) Gradient Boosting (base model)\n# (sequential trees fit residuals; LR shrinks each step)\ngbr_base = Pipeline([\n    (\"prep\", preprocess),\n    (\"gbr\", GradientBoostingRegressor(\n        learning_rate=0.05,   # smaller -> more trees often needed\n        n_estimators=600,     # number of weak learners\n        max_depth=2,          # tree depth (complexity)\n        min_samples_leaf=5,   # regularization (minimum samples per leaf)\n        subsample=0.9,        # stochastic boosting (regularization)\n        random_state=RANDOM_SEED\n    ))\n])\ngbr_base.fit(X_train, y_train)\ngbr_pred = gbr_base.predict(X_test)\ngbr_mae  = mean_absolute_error(y_test, gbr_pred)\ngbr_rmse = mean_squared_error(y_test, gbr_pred, squared=False)\nprint(f\"GradientBoost    MAE={gbr_mae:.4f}  RMSE={gbr_rmse:.4f}\")\n\n# 5) Sweep: Learning rate × Estimators\n# (Shows the LR–n_estimators tradeoff)\ndef sweep_lr_estimators(lrs=(0.2, 0.1, 0.05, 0.02), n_ests=(100, 300, 600, 1000)):\n    rows = []\n    for lr in lrs:\n        for n_est in n_ests:\n            model = Pipeline([\n                (\"prep\", preprocess),\n                (\"gbr\", GradientBoostingRegressor(\n                    learning_rate=lr,\n                    n_estimators=n_est,\n                    max_depth=2,\n                    min_samples_leaf=5,\n                    subsample=0.9,\n                    random_state=RANDOM_SEED\n                ))\n            ])\n            model.fit(X_train, y_train)\n            pred = model.predict(X_test)\n            mae = mean_absolute_error(y_test, pred)\n            rows.append({\"learning_rate\": lr, \"n_estimators\": n_est, \"MAE\": mae})\n    return pd.DataFrame(rows).sort_values([\"learning_rate\", \"n_estimators\"])\n\nsweep_lr_ne = sweep_lr_estimators()\nprint(\"\\nLR × Estimators sweep (lower MAE is better):\")\nprint(sweep_lr_ne.to_string(index=False))\n\n# Plot: for each LR, MAE vs n_estimators\nfor lr in sorted(sweep_lr_ne[\"learning_rate\"].unique()):\n    sub = sweep_lr_ne[sweep_lr_ne[\"learning_rate\"] == lr].sort_values(\"n_estimators\")\n    plt.figure(figsize=(6,3.5))\n    plt.plot(sub[\"n_estimators\"], sub[\"MAE\"], marker=\"o\")\n    plt.title(f\"MAE vs n_estimators (learning_rate={lr})\")\n    plt.xlabel(\"n_estimators\")\n    plt.ylabel(\"MAE\")\n    plt.grid(True, linestyle=\"--\", alpha=0.5)\n    plt.show()\n\n# 6) Sweep: Tree depth + Regularization\n# (Depth = weak learner complexity; min_samples_leaf/subsample = regularization)\ndef sweep_depth_reg(depths=(1,2,3), leaves=(1,5,10), subs=(1.0, 0.9, 0.7), lr=0.05, n_est=600):\n    rows = []\n    for d in depths:\n        for leaf in leaves:\n            for sub in subs:\n                model = Pipeline([\n                    (\"prep\", preprocess),\n                    (\"gbr\", GradientBoostingRegressor(\n                        learning_rate=lr,\n                        n_estimators=n_est,\n                        max_depth=d,\n                        min_samples_leaf=leaf,\n                        subsample=sub,\n                        random_state=RANDOM_SEED\n                    ))\n                ])\n                model.fit(X_train, y_train)\n                pred = model.predict(X_test)\n                mae = mean_absolute_error(y_test, pred)\n                rows.append({\n                    \"max_depth\": d,\n                    \"min_samples_leaf\": leaf,\n                    \"subsample\": sub,\n                    \"MAE\": mae\n                })\n    return pd.DataFrame(rows).sort_values([\"max_depth\",\"min_samples_leaf\",\"subsample\"])\n\nsweep_reg = sweep_depth_reg()\nprint(\"\\nDepth + Regularization sweep (lower MAE is better):\")\nprint(sweep_reg.to_string(index=False))\n\n# Plots: one per depth; MAE vs min_samples_leaf, grouped by subsample\nfor d in sorted(sweep_reg[\"max_depth\"].unique()):\n    subd = sweep_reg[sweep_reg[\"max_depth\"] == d]\n    pivot = subd.pivot_table(index=\"min_samples_leaf\", columns=\"subsample\", values=\"MAE\", aggfunc=\"mean\")\n    plt.figure(figsize=(6,3.5))\n    for sub in pivot.columns:\n        plt.plot(pivot.index, pivot[sub], marker=\"o\", label=f\"subsample={sub}\")\n    plt.title(f\"MAE vs min_samples_leaf (max_depth={d})\")\n    plt.xlabel(\"min_samples_leaf\")\n    plt.ylabel(\"MAE\")\n    plt.grid(True, linestyle=\"--\", alpha=0.5)\n    plt.legend()\n    plt.show()\n\n# 7) Final model + Feature Importance (Permutation)\nfinal_cfg = {\n    \"learning_rate\": 0.05,\n    \"n_estimators\": 800,\n    \"max_depth\": 2,\n    \"min_samples_leaf\": 5,\n    \"subsample\": 0.9\n}\nfinal_gbr = Pipeline([\n    (\"prep\", preprocess),\n    (\"gbr\", GradientBoostingRegressor(random_state=RANDOM_SEED, **final_cfg))\n])\nfinal_gbr.fit(X_train, y_train)\npred_final = final_gbr.predict(X_test)\nmae_final  = mean_absolute_error(y_test, pred_final)\nrmse_final = mean_squared_error(y_test, pred_final, squared=False)\nprint(f\"\\nFINAL GBR (cfg={final_cfg})  MAE={mae_final:.4f}  RMSE={rmse_final:.4f}\")\n\n# Permutation importance works on the trained regressor and transformed X\nprep = final_gbr.named_steps[\"prep\"]\nreg  = final_gbr.named_steps[\"gbr\"]\n\nX_test_trans = prep.transform(X_test)\n\n# Build feature names (numeric + one-hot categories)\nfeat_names = []\nfeat_names += num_cols\nif len(cat_cols) > 0:\n    ohe = prep.named_transformers_[\"cat\"].named_steps[\"ohe\"]\n    for col, cats in zip(cat_cols, ohe.categories_):\n        feat_names += [f\"{col}={c}\" for c in cats]\n\nperm = permutation_importance(reg, X_test_trans, y_test, n_repeats=5, random_state=RANDOM_SEED, n_jobs=-1)\nimportances = pd.DataFrame({\n    \"feature\": feat_names,\n    \"importance_mean\": perm.importances_mean,\n    \"importance_std\": perm.importances_std\n}).sort_values(\"importance_mean\", ascending=False)\n\nprint(\"\\nTop features (permutation importance):\")\nprint(importances.head(12).to_string(index=False))\n\nplt.figure(figsize=(7,4))\ntopk = importances.head(10).iloc[::-1]\nplt.barh(topk[\"feature\"], topk[\"importance_mean\"])\nplt.title(\"Top 10 features (Permutation Importance)\")\nplt.xlabel(\"Decrease in score\")\nplt.tight_layout()\nplt.show()\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "Matplotlib is building the font cache; this may take a moment.\n"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Using synthetic data: (1500, 3) Target = price\nNumeric columns: ['sqft', 'bed']\nCategorical columns: ['loc']\n"
        },
        {
          "ename": "<class 'TypeError'>",
          "evalue": "got an unexpected keyword argument 'squared'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 72\u001b[0m\n\u001b[1;32m     70\u001b[0m y_pred_mean \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfull_like(np\u001b[38;5;241m.\u001b[39masarray(y_test, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m), fill_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean(y_train)))\n\u001b[1;32m     71\u001b[0m mae_mean  \u001b[38;5;241m=\u001b[39m mean_absolute_error(y_test, y_pred_mean)\n\u001b[0;32m---> 72\u001b[0m rmse_mean \u001b[38;5;241m=\u001b[39m \u001b[43mmean_squared_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msquared\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBaseline (mean)  MAE=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmae_mean\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  RMSE=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrmse_mean\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     75\u001b[0m rf \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[1;32m     76\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprep\u001b[39m\u001b[38;5;124m\"\u001b[39m, preprocess),\n\u001b[1;32m     77\u001b[0m     (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m, RandomForestRegressor(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     83\u001b[0m     ))\n\u001b[1;32m     84\u001b[0m ])\n",
            "File \u001b[0;32m/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:194\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n\u001b[0;32m--> 194\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_sig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m params\u001b[38;5;241m.\u001b[39mapply_defaults()\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# ignore self/cls and positional/keyword markers\u001b[39;00m\n",
            "File \u001b[0;32m/lib/python312.zip/inspect.py:3273\u001b[0m, in \u001b[0;36mSignature.bind\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   3269\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get a BoundArguments object, that maps the passed `args`\u001b[39;00m\n\u001b[1;32m   3270\u001b[0m \u001b[38;5;124;03m    and `kwargs` to the function's signature.  Raises `TypeError`\u001b[39;00m\n\u001b[1;32m   3271\u001b[0m \u001b[38;5;124;03m    if the passed arguments can not be bound.\u001b[39;00m\n\u001b[1;32m   3272\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3273\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bind\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/lib/python312.zip/inspect.py:3262\u001b[0m, in \u001b[0;36mSignature._bind\u001b[0;34m(self, args, kwargs, partial)\u001b[0m\n\u001b[1;32m   3252\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   3253\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgot some positional-only arguments passed as \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   3254\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeyword arguments: \u001b[39m\u001b[38;5;132;01m{arg!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3259\u001b[0m             ),\n\u001b[1;32m   3260\u001b[0m         )\n\u001b[1;32m   3261\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3262\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   3263\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgot an unexpected keyword argument \u001b[39m\u001b[38;5;132;01m{arg!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   3264\u001b[0m                 arg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(kwargs))))\n\u001b[1;32m   3266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_arguments_cls(\u001b[38;5;28mself\u001b[39m, arguments)\n",
            "\u001b[0;31mTypeError\u001b[0m: got an unexpected keyword argument 'squared'"
          ],
          "output_type": "error"
        }
      ],
      "execution_count": 1
    }
  ]
}